{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "from src import utils, nmf_utils, lda_utils\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.nmf import Nmf\n",
    "\n",
    "from collections import Counter\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF - Wall Street Forum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_vectorizer(docs_raw, min_doc_freq, max_doc_freq, max_feats, ngram_rng):\n",
    "    '''\n",
    "    \n",
    "    Args:\n",
    "        \n",
    "    Return:\n",
    "    \n",
    "    '''\n",
    "    vectorizer = CountVectorizer(min_df=min_doc_freq, max_df=max_doc_freq, max_features=max_feats, ngram_range=ngram_rng)\n",
    "    docs_vectorized = vectorizer.fit_transform(docs_raw)\n",
    "    return docs_vectorized, vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_vectorizer(docs_raw, min_doc_freq, max_doc_freq, max_feats, ngram_rng):\n",
    "    '''\n",
    "    \n",
    "    Args:\n",
    "        \n",
    "    Return:\n",
    "    \n",
    "    '''\n",
    "    vectorizer = TfidfVectorizer(min_df=min_doc_freq, max_df=max_doc_freq, max_features=max_feats, ngram_range=ngram_rng)\n",
    "    docs_vectorized = vectorizer.fit_transform(docs_raw)\n",
    "    return docs_vectorized, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    '''\n",
    "   \n",
    "    Args:\n",
    "    \n",
    "        \n",
    "    Return:\n",
    "    \n",
    "        \n",
    "    '''\n",
    "    \n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_words_arr = [words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        print(\"\\nTopic #{}:\".format(topic_idx))\n",
    "        print(\" \".join(top_words_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wsm = Wall Street Market\n",
    "# append_to_stopwords = ['wsm']\n",
    "\n",
    "stop_words = stopwords.words(\"english\")# .append(append_to_stopwords)\n",
    "stopwords_dict = Counter(stop_words)\n",
    "\n",
    "\n",
    "def tokenize_lemmatize(text):\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        comment (str) : Content of the author's post in Wall Street Market.\n",
    "        \n",
    "    Return:\n",
    "        lem_tokens (arr) : The cleaned, tokenized and lemmetized version of comment.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stopwords_dict]\n",
    "    \n",
    "    wordnet_lemma = nltk.WordNetLemmatizer()\n",
    "    lem_tokens = [wordnet_lemma.lemmatize(t) for t in tokens]\n",
    "    \n",
    "    return lem_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset and Model Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wallstreet = pd.read_csv('data/wallstreet_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must perform again b/c tokens are transformed to strings when saved to \"data/wallstreet_master.csv\" in \"wallstreet_feat_eng_eda.ipynb\"\n",
    "wallstreet['tokens_for_nmf'] = wallstreet['cleaned_text'].apply(tokenize_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postID</th>\n",
       "      <th>threadID</th>\n",
       "      <th>threadTitle</th>\n",
       "      <th>subforum</th>\n",
       "      <th>authorName</th>\n",
       "      <th>postAuthorMembership</th>\n",
       "      <th>authorReputation</th>\n",
       "      <th>postSequence</th>\n",
       "      <th>flatContent</th>\n",
       "      <th>contentWithHTMLTag</th>\n",
       "      <th>post_date_dt</th>\n",
       "      <th>author_join_date</th>\n",
       "      <th>num_days_member_when_posted</th>\n",
       "      <th>wordcloud_text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>tokens_for_nmf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>Hi</td>\n",
       "      <td>Introductions</td>\n",
       "      <td>Punka421</td>\n",
       "      <td>New member</td>\n",
       "      <td>-3</td>\n",
       "      <td>1</td>\n",
       "      <td>\\nJust thought I'd introduce myself. I am new ...</td>\n",
       "      <td>&lt;div class=\"entry-content\"&gt;\\n&lt;p&gt;Just thought I...</td>\n",
       "      <td>2016-10-26 13:58:36</td>\n",
       "      <td>2016-10-26 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thought id introduce new communities trying le...</td>\n",
       "      <td>just thought id introduce myself i am new to ...</td>\n",
       "      <td>['thought', 'id', 'introduce', 'new', 'communi...</td>\n",
       "      <td>[thought, id, introduce, new, community, tryin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>Hi</td>\n",
       "      <td>Introductions</td>\n",
       "      <td>WSM</td>\n",
       "      <td>Administrator</td>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>\\nHello  Nice to see you here!Regards\\n</td>\n",
       "      <td>&lt;div class=\"entry-content\"&gt;\\n&lt;p&gt;Hello &lt;img alt...</td>\n",
       "      <td>2016-10-26 14:04:04</td>\n",
       "      <td>2016-10-02 00:00:00</td>\n",
       "      <td>24.0</td>\n",
       "      <td>hello nice see hereregards</td>\n",
       "      <td>hello  nice to see you hereregards</td>\n",
       "      <td>['hello', 'nice', 'see', 'hereregards']</td>\n",
       "      <td>[hello, nice, see, hereregards]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>WSM Updates - Changelog (Page 1 of 4)</td>\n",
       "      <td>Announcements</td>\n",
       "      <td>WSM</td>\n",
       "      <td>Administrator</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>\\nHello everyone.I would like to tell you that...</td>\n",
       "      <td>&lt;div class=\"entry-content\"&gt;\\n&lt;p&gt;Hello everyone...</td>\n",
       "      <td>2016-10-26 16:54:27</td>\n",
       "      <td>2016-10-02 00:00:00</td>\n",
       "      <td>24.0</td>\n",
       "      <td>hello everyonei would like tell weve implement...</td>\n",
       "      <td>hello everyonei would like to tell you that w...</td>\n",
       "      <td>['hello', 'everyonei', 'would', 'like', 'tell'...</td>\n",
       "      <td>[hello, everyonei, would, like, tell, weve, im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>Hi</td>\n",
       "      <td>Introductions</td>\n",
       "      <td>Estrazy</td>\n",
       "      <td>Banned</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>\\nHello Punka! nice to meet you!As you asked h...</td>\n",
       "      <td>&lt;div class=\"entry-content\"&gt;\\n&lt;p&gt;Hello Punka! n...</td>\n",
       "      <td>2016-10-27 14:00:16</td>\n",
       "      <td>2016-10-27 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>hello punka nice meet youas asked stay secure ...</td>\n",
       "      <td>hello punka nice to meet youas you asked how ...</td>\n",
       "      <td>['hello', 'punka', 'nice', 'meet', 'youas', 'a...</td>\n",
       "      <td>[hello, punka, nice, meet, youas, asked, stay,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>WSM Updates - Changelog (Page 1 of 4)</td>\n",
       "      <td>Announcements</td>\n",
       "      <td>WSM</td>\n",
       "      <td>Administrator</td>\n",
       "      <td>74</td>\n",
       "      <td>2</td>\n",
       "      <td>\\nChangelog from Wednesday, 2nd November 2016F...</td>\n",
       "      <td>&lt;div class=\"entry-content\"&gt;\\n&lt;h5&gt;Changelog fro...</td>\n",
       "      <td>2016-11-02 15:42:27</td>\n",
       "      <td>2016-10-02 00:00:00</td>\n",
       "      <td>31.0</td>\n",
       "      <td>changelog wednesday november lot small issues ...</td>\n",
       "      <td>changelog from wednesday  november  a lot of ...</td>\n",
       "      <td>['changelog', 'wednesday', 'november', 'lot', ...</td>\n",
       "      <td>[changelog, wednesday, november, lot, small, i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   postID  threadID                            threadTitle       subforum  \\\n",
       "0       6         5                                     Hi  Introductions   \n",
       "1       7         5                                     Hi  Introductions   \n",
       "2       8         6  WSM Updates - Changelog (Page 1 of 4)  Announcements   \n",
       "3      11         5                                     Hi  Introductions   \n",
       "4      13         6  WSM Updates - Changelog (Page 1 of 4)  Announcements   \n",
       "\n",
       "  authorName postAuthorMembership  authorReputation  postSequence  \\\n",
       "0   Punka421           New member                -3             1   \n",
       "1        WSM        Administrator                66             2   \n",
       "2        WSM        Administrator                74             1   \n",
       "3    Estrazy               Banned                 0             3   \n",
       "4        WSM        Administrator                74             2   \n",
       "\n",
       "                                         flatContent  \\\n",
       "0  \\nJust thought I'd introduce myself. I am new ...   \n",
       "1            \\nHello  Nice to see you here!Regards\\n   \n",
       "2  \\nHello everyone.I would like to tell you that...   \n",
       "3  \\nHello Punka! nice to meet you!As you asked h...   \n",
       "4  \\nChangelog from Wednesday, 2nd November 2016F...   \n",
       "\n",
       "                                  contentWithHTMLTag         post_date_dt  \\\n",
       "0  <div class=\"entry-content\">\\n<p>Just thought I...  2016-10-26 13:58:36   \n",
       "1  <div class=\"entry-content\">\\n<p>Hello <img alt...  2016-10-26 14:04:04   \n",
       "2  <div class=\"entry-content\">\\n<p>Hello everyone...  2016-10-26 16:54:27   \n",
       "3  <div class=\"entry-content\">\\n<p>Hello Punka! n...  2016-10-27 14:00:16   \n",
       "4  <div class=\"entry-content\">\\n<h5>Changelog fro...  2016-11-02 15:42:27   \n",
       "\n",
       "      author_join_date  num_days_member_when_posted  \\\n",
       "0  2016-10-26 00:00:00                          0.0   \n",
       "1  2016-10-02 00:00:00                         24.0   \n",
       "2  2016-10-02 00:00:00                         24.0   \n",
       "3  2016-10-27 00:00:00                          0.0   \n",
       "4  2016-10-02 00:00:00                         31.0   \n",
       "\n",
       "                                      wordcloud_text  \\\n",
       "0  thought id introduce new communities trying le...   \n",
       "1                         hello nice see hereregards   \n",
       "2  hello everyonei would like tell weve implement...   \n",
       "3  hello punka nice meet youas asked stay secure ...   \n",
       "4  changelog wednesday november lot small issues ...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0   just thought id introduce myself i am new to ...   \n",
       "1                hello  nice to see you hereregards    \n",
       "2   hello everyonei would like to tell you that w...   \n",
       "3   hello punka nice to meet youas you asked how ...   \n",
       "4   changelog from wednesday  november  a lot of ...   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0  ['thought', 'id', 'introduce', 'new', 'communi...   \n",
       "1            ['hello', 'nice', 'see', 'hereregards']   \n",
       "2  ['hello', 'everyonei', 'would', 'like', 'tell'...   \n",
       "3  ['hello', 'punka', 'nice', 'meet', 'youas', 'a...   \n",
       "4  ['changelog', 'wednesday', 'november', 'lot', ...   \n",
       "\n",
       "                                      tokens_for_nmf  \n",
       "0  [thought, id, introduce, new, community, tryin...  \n",
       "1                    [hello, nice, see, hereregards]  \n",
       "2  [hello, everyonei, would, like, tell, weve, im...  \n",
       "3  [hello, punka, nice, meet, youas, asked, stay,...  \n",
       "4  [changelog, wednesday, november, lot, small, i...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wallstreet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_raw = wallstreet['tokens_for_nmf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOW and Tf-Idf variables\n",
    "min_doc_freq = 0.1\n",
    "max_doc_freq = 0.85\n",
    "max_feats = 10000\n",
    "ngram_rng = [1, 2]\n",
    "\n",
    "# NMF variables\n",
    "num_topics = 20\n",
    "init = 'nndsvd'\n",
    "distance_method = 'frobenius'\n",
    "regularization = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Use Gensim to Identify Optimal Number of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_dict = Dictionary(docs_raw)\n",
    "gensim_dict.filter_extremes(no_below=0.1, no_above=0.85, keep_n=10000)\n",
    "\n",
    "corpus = [gensim_dict.doc2bow(doc) for doc in docs_raw]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 2),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1),\n",
       "  (21, 2),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1)],\n",
       " [(27, 1), (28, 1), (29, 1)],\n",
       " [(1, 1),\n",
       "  (5, 1),\n",
       "  (11, 2),\n",
       "  (12, 2),\n",
       "  (13, 1),\n",
       "  (14, 2),\n",
       "  (19, 1),\n",
       "  (27, 1),\n",
       "  (30, 2),\n",
       "  (31, 1),\n",
       "  (32, 1),\n",
       "  (33, 1),\n",
       "  (34, 1),\n",
       "  (35, 1),\n",
       "  (36, 1),\n",
       "  (37, 1),\n",
       "  (38, 1),\n",
       "  (39, 1),\n",
       "  (40, 1),\n",
       "  (41, 1),\n",
       "  (42, 4),\n",
       "  (43, 1),\n",
       "  (44, 1),\n",
       "  (45, 1),\n",
       "  (46, 1),\n",
       "  (47, 1),\n",
       "  (48, 1),\n",
       "  (49, 1),\n",
       "  (50, 1),\n",
       "  (51, 2),\n",
       "  (52, 2),\n",
       "  (53, 1),\n",
       "  (54, 1),\n",
       "  (55, 2),\n",
       "  (56, 1),\n",
       "  (57, 3),\n",
       "  (58, 1),\n",
       "  (59, 1),\n",
       "  (60, 1),\n",
       "  (61, 2),\n",
       "  (62, 1),\n",
       "  (63, 1),\n",
       "  (64, 1),\n",
       "  (65, 1),\n",
       "  (66, 1),\n",
       "  (67, 1),\n",
       "  (68, 1),\n",
       "  (69, 2),\n",
       "  (70, 1),\n",
       "  (71, 1),\n",
       "  (72, 2),\n",
       "  (73, 1),\n",
       "  (74, 1),\n",
       "  (75, 1),\n",
       "  (76, 1),\n",
       "  (77, 3),\n",
       "  (78, 1),\n",
       "  (79, 1),\n",
       "  (80, 1),\n",
       "  (81, 1),\n",
       "  (82, 1),\n",
       "  (83, 1),\n",
       "  (84, 1),\n",
       "  (85, 1),\n",
       "  (86, 1),\n",
       "  (87, 1),\n",
       "  (88, 1),\n",
       "  (89, 1),\n",
       "  (90, 1),\n",
       "  (91, 1),\n",
       "  (92, 1),\n",
       "  (93, 1),\n",
       "  (94, 1),\n",
       "  (95, 1),\n",
       "  (96, 1),\n",
       "  (97, 1),\n",
       "  (98, 1),\n",
       "  (99, 1),\n",
       "  (100, 1),\n",
       "  (101, 1),\n",
       "  (102, 1),\n",
       "  (103, 1),\n",
       "  (104, 1),\n",
       "  (105, 1),\n",
       "  (106, 1),\n",
       "  (107, 1)],\n",
       " [(0, 1),\n",
       "  (18, 1),\n",
       "  (21, 1),\n",
       "  (24, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (34, 1),\n",
       "  (67, 1),\n",
       "  (108, 1),\n",
       "  (109, 1),\n",
       "  (110, 1),\n",
       "  (111, 1),\n",
       "  (112, 1),\n",
       "  (113, 1),\n",
       "  (114, 1),\n",
       "  (115, 1),\n",
       "  (116, 1),\n",
       "  (117, 2)],\n",
       " [(5, 1),\n",
       "  (19, 1),\n",
       "  (35, 1),\n",
       "  (47, 1),\n",
       "  (53, 1),\n",
       "  (69, 1),\n",
       "  (74, 1),\n",
       "  (76, 1),\n",
       "  (77, 3),\n",
       "  (80, 2),\n",
       "  (88, 1),\n",
       "  (90, 1),\n",
       "  (91, 1),\n",
       "  (92, 1),\n",
       "  (94, 1),\n",
       "  (96, 1),\n",
       "  (99, 1),\n",
       "  (100, 1),\n",
       "  (104, 1),\n",
       "  (105, 1),\n",
       "  (118, 1),\n",
       "  (119, 1),\n",
       "  (120, 1),\n",
       "  (121, 1),\n",
       "  (122, 1),\n",
       "  (123, 1),\n",
       "  (124, 1),\n",
       "  (125, 1),\n",
       "  (126, 1),\n",
       "  (127, 1),\n",
       "  (128, 1),\n",
       "  (129, 1),\n",
       "  (130, 2),\n",
       "  (131, 1),\n",
       "  (132, 2),\n",
       "  (133, 1),\n",
       "  (134, 1),\n",
       "  (135, 1),\n",
       "  (136, 1),\n",
       "  (137, 1),\n",
       "  (138, 1),\n",
       "  (139, 1),\n",
       "  (140, 1),\n",
       "  (141, 1),\n",
       "  (142, 1),\n",
       "  (143, 1),\n",
       "  (144, 1),\n",
       "  (145, 1),\n",
       "  (146, 1),\n",
       "  (147, 1),\n",
       "  (148, 1),\n",
       "  (149, 1)]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topics_sorted_by_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-dc8e69414536>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mtopics_sorted_by_co_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopics_sorted_by_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'topics_sorted_by_score' is not defined"
     ]
    }
   ],
   "source": [
    "num_topics_rng = list(np.arange(5, 75 + 1, 5)) \n",
    "coherence_scores = []\n",
    "\n",
    "# NMF MODEL PARAMS\n",
    "num_passes_over_training_docs = 5\n",
    "gradient_desc_step = 0.1\n",
    "\n",
    "# the \"normalized\" arg must equal true to be relevant.\n",
    "floor_probs_for_topics = 0.01\n",
    "normalize = True\n",
    "num_training_docs_per_chunk = 5000\n",
    "\n",
    "# W = topics by words\n",
    "w_max_iter=300\n",
    "w_stop_condition=0.0001\n",
    "\n",
    "# H = articles by topics\n",
    "h_max_iter=100\n",
    "h_stop_condition=0.001\n",
    "eval_every=10\n",
    "\n",
    "for num in num_topics_rng:\n",
    "    nmf = Nmf(corpus=corpus, num_topics=num, id2word=gensim_dict, chunksize=num_training_docs_per_chunk,  passes=num_passes_over_training_docs, kappa=gradient_desc_step,\n",
    "             normalize=normalize, minimum_probability=floor_probs_for_topics, w_max_iter=w_max_iter, w_stop_condition=w_stop_condition, h_max_iter=h_max_iter,\n",
    "             h_stop_condition=h_stop_condition, eval_every=eval_every)\n",
    "    \n",
    "    coherence_model = CoherenceModel(model=nmf, texts=docs_raw, dictionary=gensim_dict, coherence='c_v')\n",
    "    \n",
    "    coherence_scores.append(round(coherence_model.get_coherence(), 5))\n",
    "\n",
    "scores = list(zip(num_topics_rng, coherence_scores))\n",
    "topics_sorted_by_co_score = sorted(scores, key=itemgetter(1), reverse=True)[0][0]\n",
    "\n",
    "print(topics_sorted_by_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10, 0.57119), (15, 0.54468), (5, 0.53721), (35, 0.52805), (20, 0.52619), (30, 0.52435), (25, 0.50427), (55, 0.5036), (60, 0.49503), (45, 0.49125), (65, 0.48518), (70, 0.48433), (40, 0.48411), (50, 0.48408), (75, 0.47384)]\n"
     ]
    }
   ],
   "source": [
    "topics_sorted_by_co_score = sorted(scores, key=itemgetter(1), reverse=True)\n",
    "print(topics_sorted_by_co_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## NMF - Bag of Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow_docs_vectorized,  bow_vectorizer = bow_vectorizer(docs_raw, min_doc_freq, max_doc_freq, max_feats, ngram_rng)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=min_doc_freq, max_df=max_doc_freq, max_features=max_feats, ngram_range=ngram_rng)\n",
    "docs_vectorized = vectorizer.fit_transform(docs_raw)\n",
    "nmf_bow = NMF(n_components=num_topics, init='nndsvd', beta_loss=distance_method, l1_ratio=regularization).fit(bow_docs_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topic_table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-154423b074a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mn_top_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtopic_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtopic_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnmf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_top_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'topic_table' is not defined"
     ]
    }
   ],
   "source": [
    "n_top_words = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-2b574292fa0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnmf_bow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_top_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-23-5cd8c850b711>\u001b[0m in \u001b[0;36mprint_topics\u001b[1;34m(model, count_vectorizer, n_top_words)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtopic_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mtop_words_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mn_top_words\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nTopic #{}:\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_words_arr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-5cd8c850b711>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtopic_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mtop_words_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mn_top_words\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nTopic #{}:\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_words_arr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print_topics(nmf_bow, vectorizer, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-5ee342a49470>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtopic_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnmf_bow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtop_words_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mn_top_words\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nTopic #{}:\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_words_arr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-5ee342a49470>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtopic_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnmf_bow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtop_words_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtopic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mn_top_words\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nTopic #{}:\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_words_arr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "words = vectorizer.get_feature_names()\n",
    "for topic_idx, topic in enumerate(nmf_bow.components_):\n",
    "    top_words_arr = [words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "    print(\"\\nTopic #{}:\".format(topic_idx))\n",
    "    print(\" \".join(top_words_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
